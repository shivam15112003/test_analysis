import pandas as pd
from bs4 import BeautifulSoup
import requests
import nltk
from textblob import TextBlob
from textstat import  flesch_kincaid_grade, syllable_count

# create a dictionary of article URLs and their URL IDs
data = {
    "URL": ["https://www.example.com/article1", "https://www.example.com/article2"],
    "URL ID": ["id1", "id2"]
}

# convert the dictionary to a pandas dataframe
df = pd.DataFrame(data)

# save the dataframe to an Excel file
df.to_excel("input.xlsx")
input_data = pd.read_excel("Input.xlsx")

# Define functions for text extraction and analysis
def extract_article_text(url):
    try:
        # Send a GET request to the URL
        response = requests.get(url)
        
        # Check if the request was successful
        if response.status_code == 200:
            # Parse the HTML content of the page using BeautifulSoup
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Find the article title
            article_title = soup.title.text.strip()
            
            # Find all <p> tags within the article body
            paragraphs = soup.find_all('p')
            
            # Extract text from each paragraph and concatenate them
            article_text = ' '.join([p.text.strip() for p in paragraphs])
            
            return article_title, article_text
        else:
            print(f"Failed to fetch content from {url}. Status code: {response.status_code}")
            return None, None
    except Exception as e:
        print(f"An error occurred while extracting content from {url}: {str(e)}")
        return None, None

    pass

def perform_text_analysis(article_text):
    try:
        # Tokenize the article text into sentences
        sentences = nltk.sent_tokenize(article_text)
        
        # Calculate average sentence length
        avg_sentence_length = sum(len(sentence.split()) for sentence in sentences) / len(sentences)
        
        # Calculate percentage of complex words
        complex_word_count = sum(1 for word in nltk.word_tokenize(article_text) if syllable_count(word) > 2)
        percentage_complex_words = (complex_word_count / len(nltk.word_tokenize(article_text))) * 100

        # Calculate the number of syllables per word
        syllables_per_word = [textstat.syllable_count(word) for word in  nltk.word_tokenize(article_text)]

        
        # Calculate Flesch-Kincaid Grade Level
        flesch_kincaid_grade_level = flesch_kincaid_grade(article_text)
        
        # Calculate average number of words per sentence
        avg_words_per_sentence = len(nltk.word_tokenize(article_text)) / len(sentences)
        
        # Calculate number of personal pronouns
        personal_pronouns = sum(1 for word, tag in nltk.pos_tag(nltk.word_tokenize(article_text)) if tag == 'PRP')
        
        # Calculate average word length
        word_lengths = [len(word) for word in nltk.word_tokenize(article_text)]
        avg_word_length = sum(word_lengths) / len(word_lengths)
        
        # Perform sentiment analysis using TextBlob
        blob = TextBlob(article_text)
        polarity_score = blob.sentiment.polarity
        subjectivity_score = blob.sentiment.subjectivity
        
        # Define positive and negative words
        positive_words = ['good', 'great', 'excellent', 'positive']  # Example list of positive words
        negative_words = ['bad', 'terrible', 'awful', 'negative']   # Example list of negative words
        
        # Calculate positive and negative scores
        positive_score = sum(article_text.lower().count(word) for word in positive_words)
        negative_score = sum(article_text.lower().count(word) for word in negative_words)
        
        # Calculate polarity score (positive_score - negative_score)
        polarity_score = positive_score - negative_score
        
        # Compute other variables as needed
        
        return (positive_score, negative_score, polarity_score, subjectivity_score, 
                avg_sentence_length, percentage_complex_words, flesch_kincaid_grade_level, 
                avg_words_per_sentence, complex_word_count, len(nltk.word_tokenize(article_text)), 
                syllables_per_word, personal_pronouns, avg_word_length)
    
    except Exception as e:
        print(f"An error occurred during text analysis: {str(e)}")
        return (None,) * 13  # Return a tuple of Nones if an error occurs

    pass

# Iterate through each URL and perform extraction and analysis
output_data = []
for index, row in input_data.iterrows():
    url_id = row['URL_ID']
    url = row['URL']
    
    # Extract article text
    article_title, article_text = extract_article_text(url)
    
    # Perform text analysis
    text_analysis_results = perform_text_analysis(article_text)

    text_analysis_variables=['positive_score', 'negative_score', 'polarity_score', 'subjectivity_score', 
                'avg_sentence_length', 'percentage_complex_words', 'flesch_kincaid_grade_level', 
                'avg_words_per_sentence', 'complex_word_count', 'WORD_COUNT', 
                'syllable_per_word', 'personal_pronouns', 'avg_word_length']

    
    # Append results to output_data
    output_data.append([url_id, url, article_title, article_text] + text_analysis_results)

# Create DataFrame for output
output_columns=[url_id, url, article_title, article_text] + text_analysis_results
output_df = pd.DataFrame(output_data, columns=output_columns)

# Write output to Excel or CSV file
output_df.to_excel()("Output.xlsx", index=False)  # or output_df.to_csv("Output.csv", index=False)
